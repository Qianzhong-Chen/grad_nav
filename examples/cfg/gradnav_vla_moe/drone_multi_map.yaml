params:  
  diff_env:
    name: DroneVLAMultiMapEnv
    stochastic_env: True
    episode_length: 500
    MM_caching_frequency: 16

  network:
    actor: TaskAwareActor # ActorDeterministicMLP
    actor_mlp:
      'obs_units': [1024, 512, 256]     # MLP for proprioception
      'vlm_units': [1024, 512, 256]     # MLP for VLM feature
      'fusion_units': [512, 256, 128]       # Final fused layer before action output
      'activation': 'elu'

    critic: TaskAwareCriticMLP
    critic_mlp:
      'obs_units': [1024, 512, 256]       # MLP for proprioception
      'vlm_units': [1024, 512, 256]       # MLP for VLM feature
      'fusion_units': [512, 256, 128]      # Final fused layer before action output
      'activation': 'elu'

  vae:
    kl_weight: 1.0
    encoder_units: [256, 256, 256]
    decoder_units: [32, 64, 128, 256]

  env_hyper:
    VLM_FEATURE_SIZE: 512
    SINGLE_VISUAL_INPUT_SIZE: 32
    HISTORY_BUFFER_NUM: 5
    LATENT_VECT_NUM: 24
    DEPTH_DATA: False

  config:
    name: drone_vla_multi_map
    map_name: gate_mid # [gate_mid, gate_left]
    actor_learning_rate: 1e-4 # adam
    critic_learning_rate: 1e-4 # adam
    vae_learning_rate: 1e-4
    vel_net_learning_rate: 1e-4
    lr_schedule: cosine # ['constant', 'linear', 'cosine']
    target_critic_alpha: 0.2
    curriculum: False
    obs_rms: True
    ret_rms: False
    critic_iterations: 16
    critic_method: td-lambda # ['td-lambda', 'one-step']
    lambda: 0.95 # init 0.95
    num_batch: 4
    gamma: 0.99 # 0.99
    betas: [0.7, 0.95] # adam
    steps_num: 32
    grad_norm: 1.0
    truncate_grads: True
    num_actors: 128
    save_interval: 100
    max_epochs: 3000 # total training iterations

    multi_map: True
    map_change_time: 100

    domain_randomization: True

    # Low Pass Filter
    LPF_train: True
    LPF_val: 0.5

    player:
      task: 0 # [0,1,2,3]; only work when num_actors = 1
      determenistic: True
      games_num: 1
      num_actors: 1
      print_stats: True
      LPF_eval: True
      LPF_val: 0.5
